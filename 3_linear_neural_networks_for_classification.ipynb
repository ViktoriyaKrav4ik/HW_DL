{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Neural Networks for Classification\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: \n",
    "\n",
    "It's primarily used for **binary classification**, meaning it deals with two possible outcomes (e.g., spam or not spam, positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat:\n",
      "[50, 50, 50, 50]\n",
      "[50, 200, 200, 50]\n",
      "[50, 200, 200, 50]\n",
      "[50, 50, 50, 50]\n",
      "Dog:\n",
      "[150, 150, 150, 150]\n",
      "[150, 150, 150, 150]\n",
      "[150, 150, 150, 150]\n",
      "[150, 150, 150, 150]\n",
      "Cat:\n",
      "[53, 45, 41, 44]\n",
      "[55, 190, 200, 57]\n",
      "[40, 208, 200, 49]\n",
      "[52, 44, 43, 55]\n",
      "Dog:\n",
      "[145, 151, 146, 143]\n",
      "[154, 157, 154, 158]\n",
      "[146, 155, 146, 159]\n",
      "[146, 142, 145, 148]\n",
      "x shape ->  (20, 4, 4)\n",
      "y shape ->  (20,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (20,) (20,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m#Create and train LogisticRegression \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m model \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mprint\u001b[39m(predictions)\n",
      "\u001b[1;32m/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_iterations):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     y_predicted \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sigmoid(np\u001b[39m.\u001b[39mdot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_loss(y, y_predicted)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00m_\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     dw, db \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_gradients(X, y, y_predicted)\n",
      "\u001b[1;32m/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute the binary cross-entropy loss.\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m m \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(y)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imax/Documents/github/neural_n00b/2_logistic_and_softmax_regressions_NN_for_classification.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mm \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(y \u001b[39m*\u001b[39;49m np\u001b[39m.\u001b[39;49mlog(y_predicted) \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y_predicted))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (20,) (20,4) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.dataset_service import mock_cat_dog_img_data\n",
    "samles = 10\n",
    "x_cat, x_dog = mock_cat_dog_img_data(samles)\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Cat:\")\n",
    "    [print(x) for x in x_cat[i]]\n",
    "    print(\"Dog:\")\n",
    "    [print(x) for x in x_dog[i]]\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.001, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute the sigmoid function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _compute_loss(self, y, y_predicted):\n",
    "        \"\"\"Compute the binary cross-entropy loss.\"\"\"\n",
    "        m = len(y)\n",
    "        return -1/m * np.sum(y * np.log(y_predicted) + (1 - y) * np.log(1 - y_predicted))\n",
    "    \n",
    "    def _compute_gradients(self, X, y, y_predicted):\n",
    "        \"\"\"Compute gradients for weights (dw) and bias (db).\"\"\"\n",
    "        num_samples = X.shape[0]\n",
    "        dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
    "        db = (1 / num_samples) * np.sum(y_predicted - y)\n",
    "        return dw, db\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the logistic regression model.\"\"\"\n",
    "        num_features = X.shape[1]\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            y_predicted = self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "            loss = self._compute_loss(y, y_predicted)\n",
    "            print(f\"Iteration {_}, Loss: {loss}\")\n",
    "            dw, db = self._compute_gradients(X, y, y_predicted)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict labels (0 or 1) for input data.\"\"\"\n",
    "        y_predicted = self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        return [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "\n",
    "\n",
    "#Binary classification cat = 0, dog = 1 \n",
    "y_cat = np.zeros(len(x_cat))\n",
    "y_dog = np.ones(len(x_dog))\n",
    "\n",
    "#Create 'x' and labels 'y'\n",
    "x = np.concatenate((x_cat, x_dog))\n",
    "y = np.concatenate((y_cat, y_dog))\n",
    "\n",
    "\n",
    "print(\"x as a 2D matrix -> \", x.shape)\n",
    "# Flatten each image 2D matrix into a 1D vector\n",
    "x = x.reshape(len(x), -1)\n",
    "\n",
    "print(\"x as a 1D vector -> \", x.shape)\n",
    "\n",
    "print(\"y shape -> \", y.shape)\n",
    "\n",
    "#shafle data\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "#Create and train LogisticRegression \n",
    "model = LogisticRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "predictions = model.predict(x)\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression (often called Multinomial Logistic Regression) \n",
    "\n",
    "Extension of logistic regression to handle multi-class classification problems, where there are more than two possible discrete outcomes (e.g., classify an image as a cat, dog, or hedgehog)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
